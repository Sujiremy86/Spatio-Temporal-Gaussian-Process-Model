{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb81f29-88bc-4047-a539-e1698c3ef896",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c8f7b4b-9ec7-4c50-b527-a5ec6b45b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import json\n",
    "import arrow\n",
    "import gpytorch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from torch.nn import Module\n",
    "from gpytorch.kernels import Kernel\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.functions import RBFCovariance\n",
    "from gpytorch.kernels.kernel import Kernel\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.means.mean import Mean                                      \n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from gpytorch.kernels import Kernel\n",
    "import seaborn as sns\n",
    "from scipy import interp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc81107-2fb8-4a05-a40e-e59deca8c656",
   "metadata": {},
   "source": [
    "## setting the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b086dcb3-7b77-4e6e-b0d3-7d4e2b7dcae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1f3ed-9552-4738-9c4f-f13d80103a2f",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02e68c25-e106-420e-b991-4c74e578edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads the input file and preprocesses it.\n",
    "\n",
    "    This function reads data from a CSV file, scales selected columns, \n",
    "    generates Morton codes for spatial data, and extracts additional \n",
    "    attributes for further processing. It prepares the data for machine \n",
    "    learning models by transforming it into tensors.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str)\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - X (torch.Tensor): The predictors - 'cols_to_scale'\n",
    "        - y (torch.Tensor): The target variable '7_days_lagged_hotspot' .\n",
    "        - dates (pd.Series): Series of dates corresponding to each record.\n",
    "        - additional_data (dict): A dictionary containing additional variables\n",
    "                                  such as 'dates', 'landkreis_id', 'Hotspot', \n",
    "                                  'lon', 'lat', and 'CombinedFeature'.\n",
    "                                  \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv('path/to/your/inputfile.csv')\n",
    "    cols_to_scale = [\n",
    "        'Faelle_neu', 'Inzidenz_7-Tage', 'AnzahlTodesfall', \n",
    "        'value_mobility_change', 'Neighbor_Faelle_neu', \n",
    "        'Neighbor_Inzidenz_7-Tage', 'Neighbor_AnzahlTodesfall'\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    dates = pd.to_datetime(df['Meldedatum'], format='%d-%m-%Y')\n",
    "    morton_codes = calculate_morton_codes(df[\"longitude\"].values, df[\"latitude\"].values)\n",
    "    df['Week'] = pd.to_datetime(df['Meldedatum'], format='%d-%m-%Y').dt.isocalendar().week\n",
    "    df['CombinedFeature'] = [f'{morton_code}_{week}' for morton_code, week in zip(morton_codes, df['Week'])]\n",
    "    \n",
    "    additional_data = {\n",
    "        \"dates\": dates,\n",
    "        \"landkreis_id\": df[\"LandKreis_id\"].values,\n",
    "        \"Hotspot\": df[\"Hotspot\"].values,\n",
    "        \"lon\": df[\"longitude\"].values,\n",
    "        \"lat\": df[\"latitude\"].values,\n",
    "        \"CombinedFeature\": df['CombinedFeature'].values\n",
    "    }\n",
    "\n",
    "\n",
    "    X = torch.tensor(df[cols_to_scale].values, dtype=torch.float32)\n",
    "    y = torch.tensor(df['7_days_lagged_hotspot'].values, dtype=torch.float32)\n",
    "\n",
    "    return X, y, dates, additional_data\n",
    "\n",
    "    \n",
    "class SpatioTemporalGPModel(gpytorch.models.ApproximateGP):\n",
    "    \n",
    "    def __init__(self, inducing_points):\n",
    "        \n",
    "         \n",
    "        \"\"\"\n",
    "            1. input -> inducing_points which is the points to speed up computations in the Gaussian Process            \n",
    "            2. Set up the variational distribution using CholeskyVariationalDistribution and the given inducing points' size.            \n",
    "            3. Set up the mean_module to use a constant mean using gpytorch.means.ConstantMean().            \n",
    "            4. spatial and temporal covariance kernel establised using scale kernel built on Radial Basis Function (RBF) kernel.\n",
    "                \n",
    "        \"\"\"       \n",
    "        \n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2)\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.spatial_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.temporal_covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \"\"\"\n",
    "            1. Extract spatial features from x, denoted as spatial_x. These are columns from index 4 to 6 ('Neighbor_Faelle_neu','Neighbor_Inzidenz_7-Tage','Neighbor_AnzahlTodesfall)            \n",
    "            2. Extract temporal features from x, denoted as temporal_x. These are columns from index 0 to 3('Faelle_neu', 'Inzidenz_7-Tage', 'AnzahlTodesfall','value_mobility_change')\n",
    "        \n",
    "        \"\"\"\n",
    "               \n",
    "        mean_x = self.mean_module(x)\n",
    "        spatial_x = x[:, 3:7]   \n",
    "        temporal_x = x[:, :4]    \n",
    "        spatial_covar_x = self.spatial_covar_module(spatial_x)\n",
    "        temporal_covar_x = self.temporal_covar_module(temporal_x)\n",
    "        covar_x = spatial_covar_x * temporal_covar_x \n",
    "        covar_x = covar_x + torch.eye(x.size(0)) * 1e-3\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "   \n",
    "def train_Bernoulli(\n",
    "    model, bernoulli_likelihood, train_loader, val_loader, n, \n",
    "    num_epochs = 16,\n",
    "    ngd_lr     = 1e-7, \n",
    "    adam_lr    = 1e-4,\n",
    "    print_iter = 100,\n",
    "    modelname  = \"STVAmean-RBFkernel\",\n",
    "    l2_lambda  = 0.1):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Trains STGP model with Bernoulli likelihood using provided data loaders.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    model : gpytorch.models.ApproximateGP  - The Gaussian Process model to be trained.\n",
    "    \n",
    "    bernoulli_likelihood : gpytorch.likelihoods.BernoulliLikelihood - Bernoulli likelihood associated with the model.\n",
    "        \n",
    "    train_loader : torch.utils.data.DataLoader - DataLoader providing batches of training data.\n",
    "        \n",
    "    val_loader : torch.utils.data.DataLoader - DataLoader providing batches of validation data.\n",
    "        \n",
    "    n : int  - Number of data points.\n",
    "        \n",
    "    num_epochs : int - Number of training epochs.\n",
    "        \n",
    "    ngd_lr : float, optional - Learning rate for Natural Gradient Descent optimizer.\n",
    "        \n",
    "    adam_lr : float, optional - Learning rate for ADAM optimizer.\n",
    "        \n",
    "    print_iter : int, optional - Frequency of printing the training progress during epochs.\n",
    "              \n",
    "    modelname : str, optional - Name for saving the trained model's weights.\n",
    "        \n",
    "    l2_lambda : float, optional - L2 regularization factor.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "    model : gpytorch.models.ApproximateGP - Trained Gaussian Process model.\n",
    "    \n",
    "    bernoulli_likelihood : gpytorch.likelihoods.BernoulliLikelihood  - Trained Bernoulli likelihood.\n",
    "    \n",
    "    train_losses : list of floats  - Training losses for each epoch.\n",
    "    \n",
    "    val_losses : list of floats  -  Validation losses for each epoch.\n",
    "\n",
    "    Note:\n",
    "    1. This function trains the STGP model with Bernoulli likelihood using Natural Gradient Descent (NGD) \n",
    "    and ADAM optimizers. Where NGD efficiently tune the variational parameters of the GP model and Adam optimizer for hyperparameters. \n",
    "    2. Training loss and validation loss are computed at each epoch, and \n",
    "    the model weights are saved. Finally, the training and validation loss curves are plotted.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    bernoulli_likelihood.train()\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(\n",
    "        model.variational_parameters(),\n",
    "        num_data=n, lr=ngd_lr)\n",
    "    \n",
    "    adam_optimizer = torch.optim.Adam([\n",
    "        {'params': model.hyperparameters()},\n",
    "        {'params': bernoulli_likelihood.parameters()},\n",
    "    ], lr=adam_lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    bernoulli_mll = gpytorch.mlls.VariationalELBO(bernoulli_likelihood, model, num_data=n)  # Evidence Lower Bound Loss function\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        epoch_train_loss = 0.0\n",
    "        for j, data in enumerate(train_loader):\n",
    "            x_batch, y_hotspot_batch = data\n",
    "            \n",
    "            adam_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            hotspot_loss = -bernoulli_mll(output, y_hotspot_batch)\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            total_loss = hotspot_loss + l2_lambda * l2_norm  # Add L2 regularization\n",
    "            epoch_train_loss += hotspot_loss.item()\n",
    "            hotspot_loss.backward()\n",
    "            adam_optimizer.step() \n",
    "\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            hotspot_loss = -bernoulli_mll(output, y_hotspot_batch)\n",
    "            hotspot_loss.backward()\n",
    "            variational_ngd_optimizer.step()\n",
    "\n",
    "            if j % print_iter == 0:\n",
    "                print(\"[%s] Epoch : %d,\\titer : %d,\\tloss : %.5e\" % (arrow.now(), i, j, hotspot_loss.item()))\n",
    "\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        bernoulli_likelihood.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_hotspot_batch in val_loader:\n",
    "                output = model(x_batch)\n",
    "                loss = -bernoulli_mll(output, y_hotspot_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"[{arrow.now()}] Epoch: {i}, Validation loss: {val_loss:.5e}\")\n",
    "        torch.save(model.state_dict(), \"saved_models/%s.pth\" % modelname)\n",
    "\n",
    "        model.train()\n",
    "        bernoulli_likelihood.train()\n",
    "    \n",
    "\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, bernoulli_likelihood, train_losses, val_losses\n",
    "    \n",
    "\n",
    "def hotspot_prediction(model, bernoulli_likelihood, data_loader):\n",
    "\n",
    "    \"\"\"\n",
    "    Predict hotspots using the given model and bernoulli likelihood.\n",
    "    \n",
    "    - This function evaluates the model on the provided data loader and returns\n",
    "    the mean of the bernoulli likelihood for each input in the data loader.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (gpytorch.models.ApproximateGP): A trained Gaussian process model.\n",
    "    - bernoulli_likelihood (gpytorch.likelihoods.BernoulliLikelihood): The Bernoulli likelihood associated with the model.\n",
    "    - data_loader (torch.utils.data.DataLoader): DataLoader containing the input data for prediction.\n",
    "    \n",
    "    Returns:\n",
    "    - means (numpy.array): The mean of the Bernoulli likelihood for each input. These values can be interpreted as the probabilities \n",
    "                           of the inputs being hotspots.\n",
    "    \n",
    "    Usage:\n",
    "    The function is used after training to make predictions on unseen data.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    bernoulli_likelihood.eval()\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        means = [ \n",
    "            bernoulli_likelihood(model(x_batch)).mean.detach().numpy() \n",
    "            for x_batch, _ in data_loader ] \n",
    "    means = np.concatenate(means, axis=0)\n",
    "    \n",
    "    return means\n",
    "    \n",
    "\n",
    "def interleave_bits(x, y):\n",
    "        \n",
    "    \"\"\"\n",
    "    Interleaves the bits of two integers. i.e. the process of alternating the bits from two integers.\n",
    "\n",
    "    This function combines the bits of two integers (x and y) in an interleaved manner. \n",
    "    It is commonly used in computing Morton codes or Z-order curve values, which are useful \n",
    "    for space-filling curves in multidimensional arrays. The function processes 32 bits from \n",
    "    each number, interleaving them into a single 64-bit integer.\n",
    "\n",
    "    Parameters:\n",
    "    x (int): The first integer representing the x-coordinate in a 2D space.\n",
    "    y (int): The second integer representing the y-coordinate in a 2D space.\n",
    "\n",
    "    Returns:\n",
    "    int: A 64-bit integer with the bits of x and y interleaved.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    z = 0\n",
    "    for i in range(32):\n",
    "        z |= (x & (1 << i)) << i | (y & (1 << i)) << (i + 1)\n",
    "    return z\n",
    "    \n",
    "\n",
    "def calculate_morton_codes(longitudes, latitudes):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Calculates Morton codes for a set of coordinates.\n",
    "\n",
    "    This function scales and converts geographic coordinates (longitudes and latitudes) to \n",
    "    integers and then computes their Morton codes. Morton codes are useful for spatial indexing.\n",
    "\n",
    "    Parameters:\n",
    "    longitudes & latitudes\n",
    "\n",
    "    Returns:\n",
    "    list: A list of Morton codes corresponding to the input coordinates.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    scaled_lons = ((longitudes - longitudes.min()) / (longitudes.max() - longitudes.min()) * 1e6).astype(int)\n",
    "    scaled_lats = ((latitudes - latitudes.min()) / (latitudes.max() - latitudes.min()) * 1e6).astype(int)\n",
    "    return [interleave_bits(lon, lat) for lon, lat in zip(scaled_lons, scaled_lats)]\n",
    "    \n",
    "\n",
    "def train_evaluate_loop(X, y, additional_data, n_splits=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains and evaluates a SpatioTemporal Gaussian Process Model using stratified k-fold cross-validation with 'combined feature (Morton encoding)' as a key.\n",
    "\n",
    "    parameters:\n",
    "        X (Tensor): Input features tensor.\n",
    "        y (Tensor): Target variable tensor.\n",
    "        additional_data (dict): Dictionary containing additional data like dates, landkreis_id, hotspot, longitude, and latitude.\n",
    "        n_splits (int): Number of splits for cross-validation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing model evaluation metrics like precision, recall, accuracy, F1 score, ROC AUC, and confusion matrices.\n",
    "\n",
    "    This function performs the following key steps:\n",
    "    \n",
    "    - Stratified K-Fold with Morton encoding splitting of the data (training, validation and test).\n",
    "    - Data resampling using SMOTE (applied only for train set) for handling class imbalance.\n",
    "    - Training a Gaussian Process Model on the training set.\n",
    "    - Evaluating the metrics like precision, recall, F1-score, ROC AUC, etc. on the validation and test sets.\n",
    "    - Collecting the result data (csv file) which contains Actual value (target variable) and Predicted value with its corresponding date, landkreis_ids, latitude and longitude. This result data has been collected to visualize the result. \n",
    "    - Returning evaluation metrics.\n",
    "\n",
    "    This function is an important part of a machine learning workflow in scenarios involving \n",
    "    spatiotemporal data, ensuring a robust evaluation of the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    gpytorch.settings.cholesky_jitter(1e-1)\n",
    "\n",
    "    all_precision_model, all_recall_model, all_accuracy_model, all_f1_model = [], [], [], []\n",
    "    all_precision_goal, all_recall_goal, all_accuracy_goal, all_f1_goal = [], [], [], []\n",
    "    all_roc_auc_model, all_roc_auc_goal = [], []\n",
    "\n",
    "    X_numpy = X.cpu().numpy()\n",
    "    y_numpy = y.cpu().numpy()\n",
    "\n",
    "    all_dates = []\n",
    "    all_y_test = []\n",
    "    all_predicted_probs = []\n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    result_data = []\n",
    "    all_test_predictions_binary = []\n",
    "    all_conf_matrices_goal = []\n",
    "    cumulative_cm_goal = np.zeros((2, 2))\n",
    "    skf = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "    for train_index, test_index in skf.split(X_numpy, additional_data['CombinedFeature']):\n",
    "        X_tr_va, X_test = X[train_index], X[test_index]\n",
    "        y_tr_va, y_test = y[train_index], y[test_index]\n",
    "        train_index, val_index = train_test_split(np.arange(len(X_tr_va)), test_size=0.20, random_state=42)\n",
    "        X_train, y_train = X_tr_va[train_index], y_tr_va[train_index]\n",
    "        X_val, y_val = X_tr_va[val_index], y_tr_va[val_index]        \n",
    "                          \n",
    "        X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "\n",
    "        X_train = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train_resampled, dtype=torch.float32)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)  \n",
    "        val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)    \n",
    "        test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)\n",
    "      \n",
    "        inducing_points = X_train[:500, :]\n",
    "        model = SpatioTemporalGPModel(inducing_points)\n",
    "        likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "       \n",
    "        model, likelihood, train_losses, val_losses = train_Bernoulli(model, likelihood, train_loader, val_loader, n=len(X_train))\n",
    "        all_train_losses.extend(train_losses)\n",
    "        all_val_losses.extend(val_losses)\n",
    "        all_dates.extend(additional_data['dates'][test_index].tolist())\n",
    "\n",
    "        test_predictions = hotspot_prediction(model, likelihood, test_loader)\n",
    "       \n",
    "        test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "    \n",
    "\n",
    "        all_test_predictions_binary.extend(test_predictions_binary.tolist())\n",
    "        y_test_actual_hotspot = additional_data['Hotspot'][test_index].astype(int)\n",
    "        cm_goal_fold = confusion_matrix(y_test_actual_hotspot, test_predictions_binary)\n",
    "        cumulative_cm_goal += cm_goal_fold\n",
    "\n",
    "        all_y_test.extend(y_test.tolist())  \n",
    "        all_predicted_probs.extend(test_predictions)\n",
    "\n",
    "        precision_model = precision_score(y_test.cpu().numpy(), test_predictions_binary)\n",
    "        recall_model = recall_score(y_test.cpu().numpy(), test_predictions_binary)\n",
    "        accuracy_model = accuracy_score(y_test.cpu().numpy(), test_predictions_binary)\n",
    "        f1_model = f1_score(y_test.cpu().numpy(), test_predictions_binary)\n",
    "\n",
    "        precision_goal = precision_score(y_test_actual_hotspot, test_predictions_binary)\n",
    "        recall_goal = recall_score(y_test_actual_hotspot, test_predictions_binary)\n",
    "        accuracy_goal = accuracy_score(y_test_actual_hotspot, test_predictions_binary)\n",
    "        f1_goal = f1_score(y_test_actual_hotspot, test_predictions_binary)\n",
    "\n",
    "        all_precision_model.append(precision_model)\n",
    "        all_recall_model.append(recall_model)\n",
    "        all_accuracy_model.append(accuracy_model)\n",
    "        all_f1_model.append(f1_model)\n",
    "\n",
    "        all_precision_goal.append(precision_goal)\n",
    "        all_recall_goal.append(recall_goal)\n",
    "        all_accuracy_goal.append(accuracy_goal)\n",
    "        all_f1_goal.append(f1_goal)\n",
    "      \n",
    "        for idx in range(len(test_predictions)):\n",
    "            index = test_index[idx]\n",
    "            result_data.append({\n",
    "                'Date': additional_data['dates'].iloc[index],\n",
    "                'Actual Value': y_test.cpu().numpy()[idx],\n",
    "                'Predicted Value': test_predictions[idx],\n",
    "                'Predicted Value Binary': test_predictions_binary[idx],\n",
    "                'Landkreis_id': additional_data['landkreis_id'][index],\n",
    "                'Hotspot': additional_data['Hotspot'][index],\n",
    "                'Longitude': additional_data['lon'][index],\n",
    "                'Latitude': additional_data['lat'][index]\n",
    "            })\n",
    "\n",
    "    avg_precision_model = np.mean(all_precision_model)\n",
    "    avg_recall_model = np.mean(all_recall_model)\n",
    "    avg_accuracy_model = np.mean(all_accuracy_model)\n",
    "    avg_f1_model = np.mean(all_f1_model)\n",
    "\n",
    "    avg_precision_goal = np.mean(all_precision_goal)\n",
    "    avg_recall_goal = np.mean(all_recall_goal)\n",
    "    avg_accuracy_goal = np.mean(all_accuracy_goal)\n",
    "    avg_f1_goal = np.mean(all_f1_goal)\n",
    "   \n",
    "    all_test_predictions_binary = np.array(all_test_predictions_binary)\n",
    "\n",
    "    all_y_test = np.array(all_y_test)\n",
    "    all_predicted_probs = np.array(all_predicted_probs)\n",
    "    avg_cm_goal = cumulative_cm_goal / n_splits\n",
    "\n",
    "    fpr_model, tpr_model, thresholds_model = roc_curve(all_y_test, all_predicted_probs)\n",
    "    roc_auc_model = auc(fpr_model, tpr_model)\n",
    "    all_roc_auc_model.append(roc_auc_model)\n",
    "\n",
    "    avg_roc_auc_model = np.mean(all_roc_auc_model)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr_model, tpr_model, color='darkorange', lw=2, label=f'Model ROC curve (area = {avg_roc_auc_model:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve - Model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Model Average Precision: {avg_precision_model:.4f}\")\n",
    "    print(f\"Model Average Recall: {avg_recall_model:.4f}\")\n",
    "    print(f\"Model Average Accuracy: {avg_accuracy_model:.4f}\")\n",
    "    print(f\"Model Average F1 Score: {avg_f1_model:.4f}\")\n",
    "    print(f\"Model Average ROC AUC: {avg_roc_auc_model:.4f}\")\n",
    "\n",
    "    print(f\"Goal Average Precision: {avg_precision_goal:.4f}\")\n",
    "    print(f\"Goal Average Recall: {avg_recall_goal:.4f}\")\n",
    "    print(f\"Goal Average Accuracy: {avg_accuracy_goal:.4f}\")\n",
    "    print(f\"Goal Average F1 Score: {avg_f1_goal:.4f}\")\n",
    "\n",
    "    cm_model = confusion_matrix(all_y_test, all_test_predictions_binary)\n",
    "    print(\"Model Confusion Matrix:\")\n",
    "    print(cm_model)\n",
    " \n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    result_df.to_csv('path/to/store/your/output/datafile.csv', index=False)\n",
    "    \n",
    "    return {\n",
    "        \"model_precision\": avg_precision_model,\n",
    "        \"model_recall\": avg_recall_model,\n",
    "        \"model_accuracy\": avg_accuracy_model,\n",
    "        \"model_f1\": avg_f1_model,\n",
    "        \"model_roc_auc\": avg_roc_auc_model,\n",
    "        \"model_confusion_matrix\": cm_model,\n",
    "        \"goal_precision\": avg_precision_goal,\n",
    "        \"goal_recall\": avg_recall_goal,\n",
    "        \"goal_accuracy\": avg_accuracy_goal,\n",
    "        \"goal_f1\": avg_f1_goal,\n",
    "        \"goal_confusion_matrix\": cumulative_cm_goal\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Main execution block to load data, train and evaluate the model, and visualize the results.\n",
    "\n",
    "    This block:\n",
    "    - Loads the data from the specified CSV file path.\n",
    "    - Preprocesses the data and splits it into features (X), labels (y), and additional data needed for the model.\n",
    "    - Trains the model using the SpatioTemporalGPModel and evaluates it using various metrics like precision, recall, accuracy, and F1-score.\n",
    "    - Visualizes the model's confusion matrix (Target variable - '7-days lagged hotspot' and its prediction) and the goal which is actual Hotspot column vs. predicted 7-days lagged hotspot -  comparison for better understanding and analysis.\n",
    "\n",
    "    The path to the input data file should be specified in place of 'path/to/your/inputfile.csv'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    X, y, dates, additional_data = load_and_preprocess_data('path/to/your/inputfile.csv')   \n",
    "    results = train_evaluate_loop(X, y, additional_data)\n",
    "    avg_precision = results[\"model_precision\"]\n",
    "    avg_recall = results[\"model_recall\"]\n",
    "    avg_accuracy = results[\"model_accuracy\"]\n",
    "    avg_f1 = results[\"model_f1\"]\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(results[\"model_confusion_matrix\"], annot=True, fmt='g', cmap='Blues', \n",
    "                xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(\"Model Confusion Matrix\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(results[\"goal_confusion_matrix\"], annot=True, fmt='g', cmap='Blues', \n",
    "                xticklabels=['Non-Hotspot', 'Hotspot'],\n",
    "                yticklabels=['Non-Hotspot', 'Hotspot'])\n",
    "    plt.title(\"actual vs predicted hotspot for next week\")\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
